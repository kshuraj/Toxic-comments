
---

### Train-Test split
* The data is split using createDatapartion from caret package    

library(caret)
set.seed(123)
train_rows=createDataPartition(data$Criminal,p=0.82,list = F)
traindata=data[train_rows,]
testdata=data[-train_rows,]


```{r}
library(DMwR)
newData <- SMOTE(Criminal ~ ., traindata, perc.over = 200,perc.under=300)
traindata=newData
table(traindata$Criminal)


#write.csv(newData,"smote.csv")
#install.packages("data.table")
library(data.table)
train$went_on_backorder <- ifelse(train$went_on_backorder == 'Yes',1,0)
test$went_on_backorder <- ifelse(test$went_on_backorder == 'Yes',1,0)
smote$went_on_backorder <- ifelse(smote$went_on_backorder == 'Yes',1,0)
```


* Standardize all the variables in the dataset     
```{r}
std_data=preProcess(traindata[,!(names(traindata)%in% "criminal")],method = c("center","scale"))
train_data=predict(std_data,traindata)
test_data=predict(std_data,testdata)

```


* Convert data into an object of the class "xgb.Dmatrix"in order to work with the xgboost model   
```{r}
library(xgboost)
train_xgb=xgb.DMatrix(data=as.matrix(train[,!(names(train) %in% "went_on_backorder")]),
                   label=as.matrix(train[,names(train)%in%"went_on_backorder"]))

test_xgb=xgb.DMatrix(data=as.matrix(test[,!(names(test) %in% "went_on_backorder")]),
                   label=as.matrix(test[,names(test) %in% "went_on_backorder"]))


smote_xgb=xgb.DMatrix(data=as.matrix(smote[,!(names(smote) %in% "went_on_backorder")]),
                   label=as.matrix(smote[,names(smote)%in%"went_on_backorder"]))

```


custom1

```{r}
xgb.max_mcc <- function(pred,train_xgb) {
  
  y_true <- getinfo(train_xgb, "label")
  
  DT <- data.table(y_true = y_true, y_prob = pred, key = "y_prob")
  cleaner <- !duplicated(DT[, "y_prob"], fromLast = TRUE)
  nump <- sum(y_true)
  numn <- length(y_true) - nump
  
  DT[, tn_v := as.numeric(cumsum(y_true == 0))]
  DT[, fp_v := cumsum(y_true == 1)]
  DT[, fn_v := numn - tn_v]
  DT[, tp_v := nump - fp_v]
  DT <- DT[cleaner, ]
  DT[, mcc := (tp_v * tn_v - fp_v * fn_v) / sqrt((tp_v + fp_v) * (tp_v + fn_v) * (tn_v + fp_v) * (tn_v + fn_v))]
  
  best_row <- which.max(DT$mcc)
  
  if (length(best_row) > 0) {
    return(list(metric = "mcc", value = DT$mcc[best_row[1]]))
  } else {
    return(list(metric = "mcc", value = -1))
  }
  
}
```

custom 2
```{r}
xgb.max_kappa <- function(pred, train_xgb) {
  
  y_true <- getinfo(train_xgb, "label")
  
  DT <- data.table(y_true = y_true, y_prob = pred, key = "y_prob")
  cleaner <- !duplicated(DT[, "y_prob"], fromLast = TRUE)
  nump <- sum(y_true)
  counter <- length(y_true)
  numn <- counter - nump
  
  DT[, tn_v := as.numeric(cumsum(y_true == 0))]
  DT[, fp_v := cumsum(y_true == 1)]
  DT[, fn_v := numn - tn_v]
  DT[, tp_v := nump - fp_v]
  DT <- DT[cleaner, ]
  DT <- DT[, pObs := (tp_v + tn_v) / counter]
  DT <- DT[, pExp := (((tp_v + fn_v) * (tp_v + fp_v)) + ((fp_v + tn_v) * (fn_v + tn_v))) / (counter * counter)]
  DT <- DT[, kappa := (pObs - pExp) / (1 - pExp)]
  
  best_row <- which.max(DT$kappa)
  
  if (length(best_row) > 0) {
    return(list(metric = "kappa", value = DT$kappa[best_row[1]]))
  } else {
    return(list(metric = "kappa", value = -1))
  }
  
}
```

### Building XGB with parameters

```{r}
set.seed(123)
params_list=list("objective"="binary:logistic","eta"=0.1,"max_depth" = 8,"colsample_bytree" = 0.5,"subsample" = 1.0,"silent" = 1,'alpha'= 4,"eval_metric" = "auc")

xgb_model_smote = xgb.cv(data =smote_xgb,params = params_list,nrounds = 1000,early_stopping_rounds = 50,nfold = 5,maximize = T) 
#xgb.max_mcc(xgb_params_pred, train_xgb)
```

```{r}


nround = 690
md <- xgb.train(data=smote_xgb, params=params_list, nrounds=nround, nthread=6)

xgb_params_pred=predict(md,test_xgb)
```


```{r} 
library(caret)
params_xgb=ifelse(xgb_params_pred<0.8
                  ,0,1)


confusionMatrix(params_xgb,test$went_on_backorder,positive = "1")

#test_new<-read.csv("criminal_test.csv") 

```

```{r}
test=test[,!(names(test)%in% c("HLCNOTYR", "IIOTHHLT", "GRPHLTIN", "IRINSUR4", "IROTHHLT", "IIINSUR4" ,"PRVHLTIN","IICHMPUS" ,"IIMEDICR", "IIMCDCHP", "HLNVREF",  "HLNVOFFR", "HLNVNEED", "HLNVCOST","IIWELMOS", "TOOLONG",  "HLCALL99" ,"AIIND102"))]
```

```{r}
test=predict(std_data,test)
test=xgb.DMatrix(data=as.matrix(test))
xgb_params_pred=predict(md,test)
params_test=ifelse(xgb_params_pred<0.5,0,1)
write.csv(params_test,"xgb14.csv")
library(h2o)
```



### variable importance
```{r fig.height=20,fig.width=12}
variable_importance_matrix=xgb.importance(feature_names = colnames(train_xgb),model=xgb_model_params)
xgb.plot.importance(variable_importance_matrix)

```


